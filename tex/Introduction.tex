\section{Introduction}\label{introduction}
In recent years, mobile devices, the digitalization of workplaces and private life, and software applications themselves have rapidly increased the amount of produced data. The amount of data has grown in several dimensions. First, the volume of data increased. Second, the variety of data has grown, e.g. there are lots of unstructured textual data created on the internet that require further processing. Third, the velocity of the produced data has sped up, as there is a growing number of devices each producing increasing volumes of data.

This development has led to new use cases for data, e.g. by enabling the use of neural networks that require lots of training data to function properly. Furthermore, whole new business models have developed solely around the usage of large amounts of data. One prominent example is the case of Cambridge Analytica~\cite{CAWashingtonPost, CANetzpolitik}, which analyzed user data on social networks to enable highly precise targeting of different demographic groups via social networks, with their working results going as far as interfering in democratic elections. Those new use cases and business models have added much value to data. But with the ongoing automization of business- as well as private tasks, the question arises, if and how all the new data can be trusted. That concern is strengthened by bots on social networks, the increased use of artificial intelligence and the fact that technology might simply fail at times and hence produce invalid data. The problem of the veracity of data is an important issue, which must be handled when working with large amounts of data.

The keywords mentioned before, i.e. volume, variety, velocity, value and veracity, define the catchword ‘big data’.

From a technical point of view, the question of how to actually process big data arises. Traditionally, relational database management systems (RDBMSs) first come to mind. While the RDMBSs are able to process large volumes of data, i.e. by parallelizing the workload, they are not a great fit. The reason is that they are not able to efficiently scale-out with increasing load, as they must replicate and restructure their databases. Additionally, because they were created for processing structured data, RDBMSs struggle with semi-structured and unstructured data. If the processed data has real time requirements, RDBMSs also suffer of a suboptimal processing speed. Beside relational systems, there are alternative database management systems which e.g. specialize in efficiently saving and querying semi- and unstructured data, often referred to as NoSQL. But those systems face similar problems regarding dynamic scale-out and general efficiency in processing. When it comes to performance, NoSQL systems might even perform worse than RDBMSs~\cite{NoSQLElephants}. A newer answer to big data and the problems of RDBMS is MapReduce~\cite{MapReduce}, which also does not necessarily show better performance than RDBMSs~\cite{RDBMSvsMapReduce}.

There are two prominent use cases for big data. The first is treating data that arrive as a continuous stream. Those data may have real time requirements and be produced by multiple sources in a heterogenous way. One example is a position tracking system aiming to locate cars and detect traffic jams, while additionally providing alternative routes in the case of a traffic jam. The second use case is the need for analyzing data of large volume, a so called batch. Such batches, with sizes ranging from several terabytes to petabytes, might be provided by relational databases, data warehouses, different devices or a combination of the aforementioned. The challenge here is to provide valid results in a reasonable time while receiving unstructured and heterogenous data. For both use cases, there is an obvious need for systems that scale, are intuitive to use, and are able to provide valid results. Traditional database systems are not a good fit for such tasks. This is the reason that many new systems have been developed around the world.

In this report we will present two prominent big data processing engines for different use cases. First (Section \ref{aurora}), we will introduce Aurora~\cite{Aurora2003}, a streaming engine for monitoring applications, which was developed in 2003. The second topic (Section \ref{stratosphere}) will be Stratosphere~\cite{Stratosphere2014}, a batch processing platform for analytical tasks published in 2014 which has since evolved into Apache Flink~\cite{Flink2015}. After explaining the need for such systems (Sections \ref{streamRDBMSProblems}, \ref{batchRDBMSProblems}), we will describe both systems in detail. Furthermore, we will discuss possible weaknesses of each system. We will also discuss the development of both projects (Section \ref{outlook}) and extend the reader's view by briefly presenting an alternative way of handling data streams (Section \ref{relatedWork}). The report is concluded (Section \ref{conclusion}) by summarizing the presented results and providing an outlook on current research topics and possible future developments.